{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_is_bad_zero(Xy_subset, min_interval=48, summer_start=3000, summer_end=7500):\n",
    "    #Summer，3000/24=125，7500/24=312.5,The 125th day to 312.5th day would be summer\n",
    "\n",
    "    meter = Xy_subset.meter_id.iloc[0]\n",
    "    is_zero = Xy_subset.meter_reading == 0 # retunr the indices of meter reading equal to 0\n",
    "    if meter == 0:\n",
    "        #electric would not be 0，so drop the row（meter equals to 0）\n",
    "        return is_zero\n",
    "\n",
    "    transitions = (is_zero != is_zero.shift(1))\n",
    "    all_sequence_ids = transitions.cumsum()\n",
    "    ids = all_sequence_ids[is_zero].rename(\"ids\")#return the meter equal to 0\n",
    "    if meter in [2, 3]:\n",
    "        # Steam and hot water could be closed\n",
    "        keep = set(ids[(Xy_subset.timestamp < summer_start) |\n",
    "                       (Xy_subset.timestamp > summer_end)].unique())#indices not in summer\n",
    "        is_bad = ids.isin(keep) & (ids.map(ids.value_counts()) >= min_interval) \n",
    "        # Get the meter data which is closed and not in the summer(at least closed 48 hours)\n",
    "    elif meter == 1:\n",
    "        time_ids = ids.to_frame().join(Xy_subset.timestamp).set_index(\"timestamp\").ids#relate ids and timestamp\n",
    "        is_bad = ids.map(ids.value_counts()) >= min_interval#at least closed 48 hours\n",
    "\n",
    "        # cold water could be closed in winter\n",
    "        jan_id = time_ids.get(0, False)#id of start from Jan\n",
    "        dec_id = time_ids.get(8283, False)#id of start from Des\n",
    "        if (jan_id and dec_id and jan_id == time_ids.get(500, False) and\n",
    "                dec_id == time_ids.get(8783, False)):\n",
    "        #If the readings for both 500 hours in January and 500 hours in December are 0\n",
    "            is_bad = is_bad & (~(ids.isin(set([jan_id, dec_id]))))\n",
    "            #Delete this part of the row from is_bad\n",
    "            \n",
    "    else:\n",
    "        raise Exception(f\"Unexpected meter type: {meter}\")\n",
    "\n",
    "    result = is_zero.copy()\n",
    "    result.update(is_bad)\n",
    "    return result\n",
    "\n",
    "def find_bad_zeros(X, y):\n",
    "    \"\"\"Return the Index that contains only the rows that should be deleted\"\"\"\n",
    "    Xy = X.assign(meter_reading=y, meter_id=X.meter)\n",
    "    is_bad_zero = Xy.groupby([\"building_id\", \"meter\"]).apply(make_is_bad_zero)\n",
    "    return is_bad_zero[is_bad_zero].index.droplevel([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_sitezero(X):\n",
    "    \"\"\"Returns the indices of the lines with abnormal readings at Site 0.\"\"\"\n",
    "    return X[(X.timestamp < 3378) & (X.site_id == 0) & (X.meter == 0)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_building1099(X, y):\n",
    "    \"\"\"Returns the indices of rows with abnormally high readings in building 1099 .\"\"\"\n",
    "    return X[(X.building_id == 1099) & (X.meter == 2) & (y > 3e4)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_rows(X, y):\n",
    "    return find_bad_zeros(X, y).union(find_bad_sitezero(X)).union(find_bad_building1099(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test():\n",
    "    df = pd.read_csv(input_file(\"test.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    #This converts timestamp to the number of hours starting at 0:00 on January 1, 2016\n",
    "    return compress_dataframe(df).set_index(\"row_id\")\n",
    "\n",
    "def read_weather_test(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    df = pd.read_csv(input_file(\"weather_test.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    #This converts timestamp to the number of hours starting at 0:00 on January 1, 2016\n",
    "    if fix_timestamps:\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "        #Unify time in different time zones\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        for site_id in df.site_id.unique():\n",
    "            # Make sure to include all possible hours, 2017 and 2018 for the entire year\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(range(8784, 26304))\n",
    "            site_df.site_id = site_id\n",
    "            for col in [c for c in site_df.columns if c != \"site_id\"]:\n",
    "                if add_na_indicators: site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "                # Fill NA with median\n",
    "                site_df[col] = site_df[col].fillna(df[col].median())\n",
    "            site_dfs.append(site_df)\n",
    "        df = pd.concat(site_dfs).reset_index()  # make timestamp back into a regular column\n",
    "    elif add_na_indicators:\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any(): df[f\"had_{col}\"] = ~df[col].isna()\n",
    " #If there is a missing value in a column, a new feature is added: had_xxx indicates whether this row has a record in the xxx column\n",
    "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
    "\n",
    "def combined_test_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    #Fill NA\n",
    "    X = compress_dataframe(read_test().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_test(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return X\n",
    "\n",
    "def combined_valid_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    X = compress_dataframe(read_test().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_test(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"max_columns\", 500)\n",
    "\n",
    "def input_file(file):\n",
    "    path = f\"./{file}\"\n",
    "    if not os.path.exists(path): return path + \".gz\"\n",
    "    return path\n",
    "\n",
    "def compress_dataframe(df):\n",
    "    '''Convert all data types to numeric'''\n",
    "    result = df.copy()\n",
    "    for col in result.columns:\n",
    "        col_data = result[col]\n",
    "        dn = col_data.dtype.name\n",
    "        if dn == \"object\":\n",
    "            result[col] = pd.to_numeric(col_data.astype(\"category\").cat.codes, downcast=\"integer\")\n",
    "        elif dn == \"bool\":\n",
    "            result[col] = col_data.astype(\"int8\")\n",
    "        elif dn.startswith(\"int\") or (col_data.round() == col_data).all():\n",
    "            result[col] = pd.to_numeric(col_data, downcast=\"integer\")\n",
    "        else:\n",
    "            result[col] = pd.to_numeric(col_data, downcast='float')\n",
    "    return result\n",
    "\n",
    "def read_train():\n",
    "    df = pd.read_csv(input_file(\"train.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    #This converts timestamp to the number of hours starting at 0:00 on January 1, 2016\n",
    "    return compress_dataframe(df)\n",
    "\n",
    "def read_building_metadata():\n",
    "    return compress_dataframe(pd.read_csv(\n",
    "        input_file(\"building_metadata.csv\")).fillna(-1)).set_index(\"building_id\")\n",
    "\n",
    "site_GMT_offsets = [-5, 0, -7, -5, -8, 0, -5, -5, -5, -6, -7, -5, 0, -6, -5, -5]\n",
    "\n",
    "def read_weather_train(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    df = pd.read_csv(input_file(\"weather_train.csv\"), parse_dates=[\"timestamp\"])\n",
    "    df.timestamp = (df.timestamp - pd.to_datetime(\"2016-01-01\")).dt.total_seconds() // 3600\n",
    "    if fix_timestamps:\n",
    "        GMT_offset_map = {site: offset for site, offset in enumerate(site_GMT_offsets)}\n",
    "        df.timestamp = df.timestamp + df.site_id.map(GMT_offset_map)\n",
    "        #According to the different time zones, the aligned time\n",
    "    if interpolate_na:\n",
    "        site_dfs = []\n",
    "        for site_id in df.site_id.unique():\n",
    "            # Make sure to include all possible hours\n",
    "            site_df = df[df.site_id == site_id].set_index(\"timestamp\").reindex(range(8784))\n",
    "            site_df.site_id = site_id\n",
    "            for col in [c for c in site_df.columns if c != \"site_id\"]:\n",
    "                if add_na_indicators: site_df[f\"had_{col}\"] = ~site_df[col].isna()\n",
    "                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "                # The missing values of some sites are filled with interpolation\n",
    "                site_df[col] = site_df[col].fillna(df[col].median())\n",
    "            site_dfs.append(site_df)\n",
    "        df = pd.concat(site_dfs).reset_index()  \n",
    "        # Before setting timestamp to index, reset index here to make timestamp return to the general column\n",
    "    elif add_na_indicators:\n",
    "        for col in df.columns:\n",
    "            if df[col].isna().any(): df[f\"had_{col}\"] = ~df[col].isna()\n",
    "    return compress_dataframe(df).set_index([\"site_id\", \"timestamp\"])\n",
    "\n",
    "def combined_train_data(fix_timestamps=True, interpolate_na=True, add_na_indicators=True):\n",
    "    Xy = compress_dataframe(read_train().join(read_building_metadata(), on=\"building_id\").join(\n",
    "        read_weather_train(fix_timestamps, interpolate_na, add_na_indicators),\n",
    "        on=[\"site_id\", \"timestamp\"]).fillna(-1))\n",
    "    return Xy.drop(columns=[\"meter_reading\"]), Xy.meter_reading\n",
    "\n",
    "def _add_time_features(X):\n",
    "    return X.assign(tm_day_of_week=((X.timestamp // 24) % 7), tm_hour_of_day=(X.timestamp % 24))\n",
    "\n",
    "categorical_columns = [\n",
    "    \"building_id\", \"meter\", \"site_id\", \"primary_use\", \"had_air_temperature\", \"had_cloud_coverage\",\n",
    "    \"had_dew_temperature\", \"had_precip_depth_1_hr\", \"had_sea_level_pressure\", \"had_wind_direction\",\n",
    "    \"had_wind_speed\", \"tm_day_of_week\", \"tm_hour_of_day\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = combined_train_data()\n",
    "\n",
    "bad_rows = find_bad_rows(X, y)\n",
    "pd.Series(bad_rows.sort_values()).to_csv(\"rows_to_drop.csv\", header=False, index=False)\n",
    "\n",
    "Xy = X.assign(meter_reading=y, meter_id=X.meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(index=bad_rows)\n",
    "y = y.reindex_like(X)\n",
    "\n",
    "# Additional preprocessing\n",
    "X = compress_dataframe(_add_time_features(X))\n",
    "X = X.drop(columns=\"timestamp\")  # drop the original timestamp\n",
    "y = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',  \n",
    "    'objective': 'regression',  \n",
    "    'metric': 'rmse',  \n",
    "    'num_leaves': 1280,  \n",
    "    'learning_rate': 0.01,  \n",
    "    #'feature_fraction': 0.8,  \n",
    "    #'bagging_fraction': 0.8,  \n",
    "    #'bagging_freq': 5,  \n",
    "    'verbose': 1,\n",
    "    #'max_depth': 8,\n",
    "    #'lambda_l1':1,\n",
    "    #'lambda_l2':3,\n",
    "}\n",
    "\n",
    "fitted={}\n",
    "for val in X['meter'].unique():\n",
    "    X1 = X[X['meter'] == val].drop(columns=['meter'])\n",
    "    kf = StratifiedKFold(n_splits=3,random_state=42)\n",
    "    #Use StratifiedKFold to make the distribution of the specified column in each fold the same, here is divided into 3 folds\n",
    "    fitted[val]=[]\n",
    "    t=0\n",
    "    for train_index,test_index in kf.split(X1,X1['tm_hour_of_day']):\n",
    "        #Make the distribution of ['tm_hour_of_day'] in each fold the same\n",
    "        train_features = X1.iloc[train_index]\n",
    "        train_target = y[X1.iloc[train_index].index]\n",
    "        \n",
    "        test_features = X1.iloc[test_index]\n",
    "        test_target = y[X1.iloc[test_index].index]\n",
    "        \n",
    "        d_train = lgb.Dataset(train_features, train_target, categorical_feature=categorical_features)\n",
    "        d_eval = lgb.Dataset(test_features,test_target, categorical_feature=categorical_features)\n",
    "        print(\"Building model meter :\",val,'fold:',t)\n",
    "        t+=1\n",
    "        \n",
    "        md = lgb.train(params, d_train, num_boost_round=20000, valid_sets=d_eval, early_stopping_rounds=500,verbose_eval=200)\n",
    "        #It can also be predicted directly below, without retaining the model, saving menmory\n",
    "        model=copy.deepcopy(md)\n",
    "        #Use deep copy to change the location where the model is saved\n",
    "        fitted[val].append(model)\n",
    "del X, y,X1,train_features,test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = combined_test_data()\n",
    "X = compress_dataframe(_add_time_features(X))\n",
    "X = X.drop(columns=\"timestamp\")  # drop the original timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros(len(X))\n",
    "for val in X['meter'].unique():\n",
    "    ix = np.nonzero((X['meter'] == val).to_numpy())\n",
    "    predictions=np.zeros([1,len(ix[0])])\n",
    "    for i in tqdm_notebook(range(len(fitted[val]))):\n",
    "        #fitted is a dictionary, fitted [val] is a list, and stores the trained model, where val stands for meter type\n",
    "        model=fitted[val][i]\n",
    "        predictions += model.predict(X.iloc[ix].drop(columns=['meter']),num_iteration=model.best_iteration)\n",
    "    result[ix] = predictions/len(fitted[val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({\n",
    "    \"row_id\": X.index,\n",
    "    \"meter_reading\": np.clip(np.expm1(result), 0, None)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "predictions.to_csv(\"cleanup_Klgb_12.csv\", index=False, float_format=\"%.4f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
